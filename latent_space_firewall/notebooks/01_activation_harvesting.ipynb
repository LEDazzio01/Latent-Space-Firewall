{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c3a349cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Environment Setup: Device=cpu | Model=gpt2-small | Target Layer=6\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import json\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from transformer_lens import HookedTransformer\n",
    "\n",
    "# Configuration\n",
    "# Pivoting to GPT-2 Small for rapid iteration and stability in Codespaces.\n",
    "MODEL_NAME = \"gpt2-small\"\n",
    "TARGET_LAYER = 6  # Middle layer (Structure: 12 layers total). \n",
    "                  # Layer 6 is optimal for capturing high-level semantic intent.\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "print(f\"üöÄ Environment Setup: Device={DEVICE} | Model={MODEL_NAME} | Target Layer={TARGET_LAYER}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1e0bc8f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading gpt2-small... (Fast load)\n",
      "Loaded pretrained model gpt2-small into HookedTransformer\n",
      "‚úÖ Model Loaded Successfully\n",
      "‚úÖ Loaded 20 prompts for harvesting.\n"
     ]
    }
   ],
   "source": [
    "# 1. Load the Hooked Transformer\n",
    "# TransformerLens was built for GPT-2, so this is the native happy path.\n",
    "try:\n",
    "    print(f\"Loading {MODEL_NAME}... (Fast load)\")\n",
    "    model = HookedTransformer.from_pretrained(\n",
    "        MODEL_NAME, \n",
    "        device=DEVICE\n",
    "    )\n",
    "    print(\"‚úÖ Model Loaded Successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Error loading {MODEL_NAME}.\")\n",
    "    raise e\n",
    "\n",
    "# 2. Load the Calibration Dataset\n",
    "# Use the absolute path for reliability\n",
    "DATA_RAW = Path('/workspaces/Latent-Space-Firewall/data/raw/ground_truth_dataset.json')\n",
    "\n",
    "if not DATA_RAW.exists():\n",
    "    # Defensive programming: Ensure the user actually ran the previous step\n",
    "    raise FileNotFoundError(f\"Could not find {DATA_RAW}. Please run 'python -m src.data_loader' first.\")\n",
    "\n",
    "with open(DATA_RAW, 'r') as f:\n",
    "    dataset = json.load(f)\n",
    "    \n",
    "print(f\"‚úÖ Loaded {len(dataset)} prompts for harvesting.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59c29fec",
   "metadata": {},
   "source": [
    "The Harvesting Loop (Layer 6 Hooks)\n",
    "\n",
    "We hook `blocks.6.hook_resid_post`. This is the exact residual stream that your FirewallEngine will need to intercept later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "53666610",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß† Harvesting Latent Vectors from Layer 6...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:02<00:00,  9.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚ú® Harvesting Complete. Tensor Shape: (20, 768) (Expected: [N, 768])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def get_activations(text, model, layer):\n",
    "    \"\"\"\n",
    "    Runs a forward pass and returns the residual stream vector \n",
    "    at the final token position for the specified layer.\n",
    "    \"\"\"\n",
    "    # Specific hook point for GPT-2\n",
    "    hook_name = f\"blocks.{layer}.hook_resid_post\"\n",
    "    \n",
    "    # Run with cache, no gradients needed\n",
    "    with torch.no_grad():\n",
    "        _, cache = model.run_with_cache(\n",
    "            text, \n",
    "            names_filter=lambda name: name == hook_name\n",
    "        )\n",
    "    \n",
    "    # Extract shape: [batch, pos, d_model] -> [1, n_tokens, 768]\n",
    "    # We grab [-1] (last token) to capture the full prompt context.\n",
    "    final_vector = cache[hook_name][0, -1, :].cpu().numpy()\n",
    "    return final_vector\n",
    "\n",
    "# Storage containers\n",
    "activations = []\n",
    "labels = []\n",
    "categories = []\n",
    "\n",
    "print(f\"üß† Harvesting Latent Vectors from Layer {TARGET_LAYER}...\")\n",
    "\n",
    "for entry in tqdm(dataset):\n",
    "    prompt = entry['text']\n",
    "    label = entry['label']      # 0 = Safe, 1 = Harmful\n",
    "    category = entry['category'] \n",
    "    \n",
    "    try:\n",
    "        # Harvest\n",
    "        vector = get_activations(prompt, model, TARGET_LAYER)\n",
    "        \n",
    "        activations.append(vector)\n",
    "        labels.append(label)\n",
    "        categories.append(category)\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Failed on prompt: '{prompt[:20]}...' | Error: {e}\")\n",
    "\n",
    "# Convert to numpy (Shape will be [N, 768] for GPT-2)\n",
    "X = np.array(activations)\n",
    "y = np.array(labels)\n",
    "meta = np.array(categories)\n",
    "\n",
    "print(f\"\\n‚ú® Harvesting Complete. Tensor Shape: {X.shape} (Expected: [N, 768])\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "144245b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíæ Artifact Saved: ../data/processed/activations.npz\n",
      "Next Step: Create 'notebooks/02_train_probe.ipynb' to build the Conformal Predictor.\n"
     ]
    }
   ],
   "source": [
    "# Save to processed directory for the Phase 2 Classifier\n",
    "OUTPUT_DIR = Path('../data/processed')\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "OUTPUT_PATH = OUTPUT_DIR / \"activations.npz\"\n",
    "\n",
    "np.savez(\n",
    "    OUTPUT_PATH, \n",
    "    activations=X, \n",
    "    labels=y, \n",
    "    categories=meta\n",
    " )\n",
    "\n",
    "print(f\"üíæ Artifact Saved: {OUTPUT_PATH}\")\n",
    "print(\"Next Step: Create 'notebooks/02_train_probe.ipynb' to build the Conformal Predictor.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
